{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15461b38",
   "metadata": {
    "id": "15461b38"
   },
   "source": [
    "## Baseline using Yolov8n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe0b1e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-21T15:45:18.210560Z",
     "iopub.status.busy": "2026-01-21T15:45:18.210149Z",
     "iopub.status.idle": "2026-01-21T15:45:25.239060Z",
     "shell.execute_reply": "2026-01-21T15:45:25.238376Z",
     "shell.execute_reply.started": "2026-01-21T15:45:18.210525Z"
    },
    "id": "afe0b1e9",
    "outputId": "173f48cd-0634-4516-b5a7-7e9831f811e1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 76.5MB/s 0.1s\n",
      "\n",
      "0: 384x640 12 persons, 1 stop sign, 2 umbrellas, 65.0ms\n",
      "Speed: 4.9ms preprocess, 65.0ms inference, 34.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "cap = cv2.VideoCapture(\"/kaggle/input/crowd-5s/crowd_5s.mp4\")\n",
    "ret, frame = cap.read()\n",
    "if ret:\n",
    "    results = model(frame)\n",
    "    annotated = results[0].plot()  # ← стандартная отрисовка YOLO\n",
    "    cv2.imwrite(\"test_frame.jpg\", annotated)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad16742",
   "metadata": {
    "id": "2ad16742"
   },
   "source": [
    "видно что есть лишние объекты и слишком жирная рамка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b77d71",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-21T15:45:25.250214Z",
     "iopub.status.idle": "2026-01-21T15:45:25.250604Z",
     "shell.execute_reply": "2026-01-21T15:45:25.250445Z",
     "shell.execute_reply.started": "2026-01-21T15:45:25.250425Z"
    },
    "id": "d3b77d71",
    "outputId": "c3756283-5825-4f61-d497-38a2f53529c5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(\"../crowd.mp4\")\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video file\")\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Resolution: {width}x{height}\")\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"Total frames: {frame_count}\")\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089cac0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3089cac0",
    "outputId": "4858a0cd-82e2-48bc-b5c9-51a73f6a13ed",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "cap = cv2.VideoCapture(\"./crowd_5s.mp4\")\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    raise ValueError(\"Failed to read frame from video!\")\n",
    "\n",
    "results = model(frame, imgsz=1280)\n",
    "boxes = results[0].boxes\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "\n",
    "for box in boxes:\n",
    "    cls_id = int(box.cls.item())\n",
    "    conf = float(box.conf.item())\n",
    "    xyxy = box.xyxy[0].cpu().numpy()\n",
    "\n",
    "    if cls_id == 0 and conf > 0.1:\n",
    "        x1, y1, x2, y2 = map(int, xyxy)\n",
    "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "        label = f\"person {conf:.2f}\"\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            label,\n",
    "            (x1, y1 - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.5,\n",
    "            color=(0, 255, 0),\n",
    "            thickness=1,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "cv2.imwrite(\"annotated_frame1.jpg\", annotated_frame)\n",
    "cap.release()\n",
    "\n",
    "print(\"✅ Frame processed and saved as 'annotated_frame.jpg'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49334e4-9379-47f9-9cf5-2382cd665abe",
   "metadata": {},
   "source": [
    "## Сравнение быстродействия трех подходов - Yolov8n , Yolov8s + SAHI , RT-DETR от Baidu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23b215e-93c5-437c-af84-09f6a76ec9bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T15:45:36.523802Z",
     "iopub.status.busy": "2026-01-21T15:45:36.523460Z",
     "iopub.status.idle": "2026-01-21T15:45:36.703877Z",
     "shell.execute_reply": "2026-01-21T15:45:36.703013Z",
     "shell.execute_reply.started": "2026-01-21T15:45:36.523772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO, RTDETR\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdea431-2b96-45ce-a19e-58aca64749b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T15:45:38.188375Z",
     "iopub.status.busy": "2026-01-21T15:45:38.187922Z",
     "iopub.status.idle": "2026-01-21T15:45:38.197094Z",
     "shell.execute_reply": "2026-01-21T15:45:38.195997Z",
     "shell.execute_reply.started": "2026-01-21T15:45:38.188333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def draw_boxes(frame, boxes, label=\"person\"):\n",
    "    \"\"\"Draw bounding boxes and labels on frame.\"\"\"\n",
    "    annotated = frame.copy()\n",
    "    for xyxy, conf in boxes:\n",
    "        x1, y1, x2, y2 = map(int, xyxy)\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        text = f\"{label} {conf:.2f}\"\n",
    "        cv2.putText(\n",
    "            annotated, text, (x1, y1 - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA\n",
    "        )\n",
    "    return annotated\n",
    "\n",
    "def extract_person_detections(results):\n",
    "    \"\"\"Extract person detections from YOLO/RT-DETR results.\"\"\"\n",
    "    boxes = []\n",
    "    for box in results[0].boxes:\n",
    "        cls_id = int(box.cls.item())\n",
    "        conf = float(box.conf.item())\n",
    "        if cls_id == 0 and conf > 0.3:\n",
    "            xyxy = box.xyxy[0].cpu().numpy()\n",
    "            boxes.append((xyxy, conf))\n",
    "    return boxes\n",
    "\n",
    "def extract_person_detections_sahi(result):\n",
    "    \"\"\"Extract person detections from SAHI result.\"\"\"\n",
    "    boxes = []\n",
    "    for obj in result.object_prediction_list:\n",
    "        if obj.category.name == \"person\":\n",
    "            conf = obj.score.value\n",
    "            if conf > 0.3:\n",
    "                x1, y1, x2, y2 = obj.bbox.to_voc_bbox()\n",
    "                boxes.append(([x1, y1, x2, y2], conf))\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c172ca-c095-4aa9-b46f-30ce581323a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T15:45:40.756532Z",
     "iopub.status.busy": "2026-01-21T15:45:40.755834Z",
     "iopub.status.idle": "2026-01-21T15:45:41.070751Z",
     "shell.execute_reply": "2026-01-21T15:45:41.070127Z",
     "shell.execute_reply.started": "2026-01-21T15:45:40.756499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"/kaggle/input/crowd-5s/crowd_5s.mp4\"\n",
    "N_FRAMES = 30\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "frames = []\n",
    "for _ in range(N_FRAMES):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae86af6-71cb-462b-a6f2-5e4b1bec9265",
   "metadata": {},
   "source": [
    "### Yolov8n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22d52f2-2006-4684-ad75-9bd71bd84297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T15:46:00.207816Z",
     "iopub.status.busy": "2026-01-21T15:46:00.207265Z",
     "iopub.status.idle": "2026-01-21T15:46:01.036098Z",
     "shell.execute_reply": "2026-01-21T15:46:01.035276Z",
     "shell.execute_reply.started": "2026-01-21T15:46:00.207784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n — avg time: 0.020s, persons in last frame: 16\n"
     ]
    }
   ],
   "source": [
    "model_yolo = YOLO(\"yolov8n.pt\")\n",
    "model_yolo.to('cuda')\n",
    "times_yolo = []\n",
    "last_detections_yolo = []\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    start = time.time()\n",
    "    results = model_yolo(frame, imgsz=1280, verbose=False)\n",
    "    boxes = extract_person_detections(results)\n",
    "    elapsed = time.time() - start\n",
    "    times_yolo.append(elapsed)\n",
    "    if i == len(frames) - 1:\n",
    "        last_detections_yolo = boxes\n",
    "        img_yolo = draw_boxes(frame, boxes)\n",
    "        cv2.imwrite(\"yolo8.jpg\", img_yolo)\n",
    "\n",
    "avg_time_yolo = sum(times_yolo) / len(times_yolo)\n",
    "print(f\"YOLOv8n — avg time: {avg_time_yolo:.3f}s, persons in last frame: {len(last_detections_yolo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79618ffb-436c-4487-b22c-2f1341edbf7a",
   "metadata": {},
   "source": [
    "### Yolo26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25614c3d-bd87-480a-bf5f-d835ed38405a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T15:38:53.795317Z",
     "iopub.status.busy": "2026-01-21T15:38:53.795002Z",
     "iopub.status.idle": "2026-01-21T15:39:02.892473Z",
     "shell.execute_reply": "2026-01-21T15:39:02.891656Z",
     "shell.execute_reply.started": "2026-01-21T15:38:53.795288Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO26l — avg time: 0.286s, persons in last frame: 20\n"
     ]
    }
   ],
   "source": [
    "model_yolo = YOLO(\"yolo26l.pt\")\n",
    "model_yolo.to('cuda')\n",
    "times_yolo = []\n",
    "last_detections_yolo = []\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    start = time.time()\n",
    "    results = model_yolo(frame, imgsz=1280, verbose=False)\n",
    "    boxes = extract_person_detections(results)\n",
    "    elapsed = time.time() - start\n",
    "    times_yolo.append(elapsed)\n",
    "    if i == len(frames) - 1:\n",
    "        last_detections_yolo = boxes\n",
    "        img_yolo = draw_boxes(frame, boxes)\n",
    "        cv2.imwrite(\"yolov26.jpg\", img_yolo)\n",
    "\n",
    "avg_time_yolo = sum(times_yolo) / len(times_yolo)\n",
    "print(f\"YOLO26l — avg time: {avg_time_yolo:.3f}s, persons in last frame: {len(last_detections_yolo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb97242-5401-490a-957c-92c1c75e9cfb",
   "metadata": {},
   "source": [
    "### Yolov8 + SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "741310e6-5f3e-48fc-93d3-635504df59cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T15:25:58.785177Z",
     "iopub.status.busy": "2026-01-21T15:25:58.784647Z",
     "iopub.status.idle": "2026-01-21T15:26:28.650788Z",
     "shell.execute_reply": "2026-01-21T15:26:28.650036Z",
     "shell.execute_reply.started": "2026-01-21T15:25:58.785147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "SAHI (YOLOv8m) — avg time: 0.990s, persons in last frame: 30\n"
     ]
    }
   ],
   "source": [
    "sahi_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"yolov8\",\n",
    "    model_path=\"yolov8s.pt\",\n",
    "    confidence_threshold=0.3,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "times_sahi = []\n",
    "last_detections_sahi = []\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    start = time.time()\n",
    "    result = get_sliced_prediction(\n",
    "        image=frame,\n",
    "        detection_model=sahi_model,\n",
    "        slice_height=512,\n",
    "        slice_width=512,\n",
    "        overlap_height_ratio=0.2,\n",
    "        overlap_width_ratio=0.2\n",
    "    )\n",
    "    boxes = extract_person_detections_sahi(result)\n",
    "    elapsed = time.time() - start\n",
    "    times_sahi.append(elapsed)\n",
    "    if i == len(frames) - 1:\n",
    "        last_detections_sahi = boxes\n",
    "        img_sahi = draw_boxes(frame, boxes)\n",
    "        cv2.imwrite(\"img_sahi.jpg\", img_sahi)\n",
    "\n",
    "\n",
    "avg_time_sahi = sum(times_sahi) / len(times_sahi)\n",
    "print(f\"SAHI (YOLOv8m) — avg time: {avg_time_sahi:.3f}s, persons in last frame: {len(last_detections_sahi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea1b11-3530-4bf7-ac6e-3eebc84a1e2c",
   "metadata": {},
   "source": [
    "### RTDETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fb964fe-0312-4a76-b7ad-8df4bc0d6bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T15:28:22.393420Z",
     "iopub.status.busy": "2026-01-21T15:28:22.392626Z",
     "iopub.status.idle": "2026-01-21T15:28:35.315509Z",
     "shell.execute_reply": "2026-01-21T15:28:35.314882Z",
     "shell.execute_reply.started": "2026-01-21T15:28:22.393372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT-DETR — avg time: 0.410s, persons in last frame: 26\n"
     ]
    }
   ],
   "source": [
    "model_rtdetr = RTDETR(\"rtdetr-l.pt\")\n",
    "model_rtdetr.to(\"cuda\")\n",
    "\n",
    "times_rtdetr = []\n",
    "last_detections_rtdetr = []\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    start = time.time()\n",
    "    results = model_rtdetr(frame, imgsz=1280, verbose=False)\n",
    "    boxes = extract_person_detections(results)  # same format as YOLO\n",
    "    elapsed = time.time() - start\n",
    "    times_rtdetr.append(elapsed)\n",
    "    if i == len(frames) - 1:\n",
    "        last_detections_rtdetr = boxes\n",
    "        img_rtdetr = draw_boxes(frame, boxes)\n",
    "        cv2.imwrite(\"img_rtdetr.jpg\", img_rtdetr)\n",
    "\n",
    "avg_time_rtdetr = sum(times_rtdetr) / len(times_rtdetr)\n",
    "print(f\"RT-DETR — avg time: {avg_time_rtdetr:.3f}s, persons in last frame: {len(last_detections_rtdetr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f861713-bfed-4354-8dfb-1ffe62952c6c",
   "metadata": {},
   "source": [
    "### RTDETR + SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c55b182-641e-4292-9e85-005f4bf1be11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T15:28:53.163036Z",
     "iopub.status.busy": "2026-01-21T15:28:53.162245Z",
     "iopub.status.idle": "2026-01-21T15:30:06.776675Z",
     "shell.execute_reply": "2026-01-21T15:30:06.775889Z",
     "shell.execute_reply.started": "2026-01-21T15:28:53.163002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "Performing prediction on 15 slices.\n",
      "SAHI (YOLOv8m) — avg time: 2.436s, persons in last frame: 37\n"
     ]
    }
   ],
   "source": [
    "sahi_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"ultralytics\",\n",
    "    model_path=\"rtdetr-l.pt\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "times_sahi = []\n",
    "last_detections_sahi = []\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    start = time.time()\n",
    "    result = get_sliced_prediction(\n",
    "        image=frame,\n",
    "        detection_model=sahi_model,\n",
    "        slice_height=512,\n",
    "        slice_width=512,\n",
    "        overlap_height_ratio=0.2,\n",
    "        overlap_width_ratio=0.2\n",
    "    )\n",
    "    boxes = extract_person_detections_sahi(result)\n",
    "    elapsed = time.time() - start\n",
    "    times_sahi.append(elapsed)\n",
    "    if i == len(frames) - 1:\n",
    "        last_detections_sahi = boxes\n",
    "        img_sahi = draw_boxes(frame, boxes)\n",
    "        cv2.imwrite(\"img_sahi_rtder.jpg\", img_sahi)\n",
    "\n",
    "\n",
    "avg_time_sahi = sum(times_sahi) / len(times_sahi)\n",
    "print(f\"SAHI (YOLOv8m) — avg time: {avg_time_sahi:.3f}s, persons in last frame: {len(last_detections_sahi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dc9a2",
   "metadata": {
    "id": "d85dc9a2"
   },
   "source": [
    "# Видеопоток\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d94faf-4bc8-4515-8bed-5cbddccec1e8",
   "metadata": {},
   "source": [
    "Для лучшего сочетания сохраним видео."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f657a485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T16:13:47.303151Z",
     "iopub.status.busy": "2026-01-21T16:13:47.302340Z",
     "iopub.status.idle": "2026-01-21T16:25:20.796563Z",
     "shell.execute_reply": "2026-01-21T16:25:20.795790Z",
     "shell.execute_reply.started": "2026-01-21T16:13:47.303116Z"
    },
    "id": "f657a485",
    "outputId": "4685c0db-1268-4e4b-a06b-cea1dc3d282e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/rtdetr-l.pt to 'rtdetr-l.pt': 100% ━━━━━━━━━━━━ 63.4MB 179.2MB/s 0.4s0.3s<0.1s\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import RTDETR\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "\n",
    "sahi_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"rtdetr\",\n",
    "    model_path=\"rtdetr-l.pt\",\n",
    "    confidence_threshold=0.35,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "input_video_path = \"/kaggle/input/full-crowd/crowd.mp4\"\n",
    "output_video_path = \"output_sahi_rtdetr.avi\"\n",
    "\n",
    "if not os.path.exists(input_video_path):\n",
    "    raise FileNotFoundError(f\"Input video not found: {input_video_path}\")\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(f\"Cannot open video: {input_video_path}\")\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "frame_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    result = get_sliced_prediction(\n",
    "        image=frame,\n",
    "        detection_model=sahi_model,\n",
    "        slice_height=512,\n",
    "        slice_width=512,\n",
    "        overlap_height_ratio=0.2,\n",
    "        overlap_width_ratio=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    boxes = extract_person_detections_sahi(result)\n",
    "    annotated_frame = draw_boxes(frame, boxes)\n",
    "    out.write(annotated_frame)\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9301907,
     "sourceId": 14562741,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9308431,
     "sourceId": 14572282,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
